---
- name: Test confirmation
  hosts: all
  gather_facts: false
  tasks:
  - name: do check
    fail: msg="you must set up a cluster yml file with cluster_confirm = prefix"
    when: prefix != cluster_confirm

- name: gather facts
  hosts: all

- name: Base configs for all servers
  tags: base
  hosts: all
  roles:
  - {role: base}

- name: Common configuration
  tags: common
  hosts: all
  roles:
  - {role: common}
  - {role: collectd}

- name: Management configuration
  tags: management
  hosts: management
  roles:
  - {role: management}
  - {role: statsd}
  - {role: graphite}
  - {role: dispersion}
  - {role: object-expirer}
  - role: ntp
    "swift_ntp_servers": "{{swift_ntp_upstream_servers}}"
    "swift_ntp_client_nets":
      - "{{swift_nets['management']}}"

# FIXME(rp): these dsh configuration should probably
# be done serial: 1 so as not to race
- name: DSH Configuration (swift)
  tags: management,dsh
  hosts: all:!management
  roles:
  - role: dsh
    dsh_server: "{{swift_dsh_host}}"
    dsh_group: swift

- name: DSH Configuration (proxy)
  tags: management,dsh
  hosts: proxy:auth-proxy
  roles:
  - role: dsh
    dsh_server: "{{swift_dsh_host}}"
    dsh_group: proxy

- name: DSH Configuration (storage)
  tags: management,dsh
  hosts: storage
  roles:
  - role: dsh
    dsh_server: "{{swift_dsh_host}}"
    dsh_group: storage

# Set up ntp
- name: NTP Configuration
  hosts: all:!management
  roles:
    - role: ntp
      swift_ntp_servers: "{{swift_ntp_local_servers}}"

# Set up syslog
- name: syslog server
  hosts: "{{swift_syslog_server_role|default('management')}}"
  tags: syslog
  roles:
  - {role: syslog, swift_syslog_role: server}

- name: syslog clients
  hosts: storage:proxy:auth-proxy
  tags: syslog
  roles:
  - {role: syslog}

# Apply sysctls
- name: proxy sysctls
  hosts: proxy:auth-proxy
  tags: sysctl
  roles:
  - role: sysctl
    conf_file: 20-proxy.conf
    sysctl_values:
      net.ipv4.tcp_tw_recycle: 1
      net.ipv4.tcp_tw_reuse: 1
      net.ipv4.ip_local_port_range: 1024 61000
      net.ipv4.tcp_syncookies: 0

- name: storage sysctls
  hosts: storage
  tags: sysctl
  roles:
  - role: sysctl
    conf_file: 30-storage.conf
    sysctl_values:
      net.ipv4.tcp_tw_recycle: 1
      net.ipv4.tcp_tw_reuse: 1
      net.ipv4.ip_local_port_range: 1024 61000
      net.ipv4.tcp_syncookies: 0
      vm.min_free_kbytes: "{{ 1048576 if ((ansible_memfree_mb * 1024)/2 > 1048576) else ((ansible_memfree_mb * 1024)/2)|int }}"

- name: Storage configuration
  tags: storage
  hosts: storage
  roles:
  - {role: storage}
  - {role: object, tags: ['object']}
  - {role: container, tags: ['container']}
  - {role: account, tags: ['account']}

- name: Proxy configuration
  tags: proxy
  hosts: proxy
  roles:
  - {role: proxy, swift_allow_account_management: false}

- name: Auth proxy configuration
  tags: proxy
  hosts: management:auth-proxy
  roles:
  - {role: proxy, swift_allow_account_management: true}

- name: Auth proxy configuration
  tags: proxy
  hosts: auth-proxy
  roles:
  - {role: proxy, swift_allow_account_management: true, swift_auth_type: liteauth}

- name: Set up ring suckage
  tags: rings
  hosts: storage:proxy:auth-proxy
  roles:
  - {role: rings, ring_master: "{{swift_ring_master_host}}"}

- name: Build rings
  tags: rings
  hosts: storage
  serial: 1
  roles:
    - {role: generate-rings, ring_master: "{{swift_ring_master_host}}", when: swift_manage_rings}

- name: Rebalance rings
  tags: rings
  hosts: management
  roles:
    - {role: rebalance-rings, ring: object, when: swift_manage_rings}
    - {role: rebalance-rings, ring: container, when: swift_manage_rings}
    - {role: rebalance-rings, ring: account, when: swift_manage_rings}

- name: Pull rings
  tags: rings
  hosts: storage:proxy:auth-proxy
  roles:
    - {role: pull-rings, ring_master: "{{prefix}}-management", when: swift_manage_rings}
  

# This is awkward... straddles the line between bootstrap and ongoing
# config management
- name: ensure storage services
  tags: services
  hosts: storage
  tasks:
    - name: ensure server services
      service: name=swift-{{item}} enabled=yes state=started
      with_items: [ "object", "container", "account" ]

    - name: ensure replicator services
      service: name=swift-{{item}}-replicator enabled=yes state=started
      with_items: [ "object", "container", "account" ]

    - name: ensure auditor services
      service: name=swift-{{item}}-auditor enabled=yes state=started
      with_items: [ "object", "container", "account" ]

    - name: ensure updater services
      service: name=swift-{{item}}-updater enabled=yes state=started
      with_items: [ "object", "container" ]

    - name: ensure account reaper
      service: name=swift-account-reaper enabled=yes state=started

- name: ensure proxy services
  tags: services
  hosts: proxy:auth-proxy:management
  tasks:
    - name: ensure proxy service
      service: name=swift-proxy enabled=yes state=started

- name: zwift configuration
  tags: zwift
  hosts: "{{swift_ui_role|default('proxy')}}"
  roles:
  - {role: zwift-ui}

- name: postfix relay config
  tags: mail
  hosts: management
  roles:
  - {role: postfix, swift_relay_net: swift}

- name: postfix configuration
  tags: mail
  hosts: proxy:storage:auth-proxy
  roles:
  - {role: postfix, swift_relay_host: "{{prefix}}-management"}

- name: haproxy configuration
  tags: lb
  hosts: "{{swift_lb_role|default('proxy')}}"
  roles:
  - {role: haproxy}

- name: management monitoring configuration
  tags: monitoring
  hosts: management
  roles:
  - role: collectd-exec
    "swift_collectd_script_dir": "/usr/share/collectd-scripts"
    "swift_collectd_scripts":
      - monitor-dispersion
      - monitor-md5
      - monitor-quarantine
      - monitor-umount
    "swift_collectd_exec_conf": "collectd-management.conf"
